{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abundance Train and Test (R2)\n",
    "\n",
    "**Written by Timm Nawrocki**\n",
    "\n",
    "*Last updated Saturday, May 17, 2020.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---------------------------------------------------------------------------\n",
    "# Distribution-abundance Train and Test\n",
    "# Author: Timm Nawrocki, Alaska Center for Conservation Science\n",
    "# Created on: 2020-05-14\n",
    "# Usage: Must be executed as a Jupyter Notebook in an Anaconda 3 installation.\n",
    "# Description: \"Distribution-Abundance Train and Test\" trains a classifier to predict species presence and absence and trains a regressor to predict species abundance within areas of predicted presence. The predictions are composited into a single continuous output that can theoretically range from 0 to 100 representing percent cover. All model performance metrics are calculated on independent test partitions.\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This script runs the model train and test steps to output a model performance and variable importance report, trained classifier file, trained regressor file, and threshold files that can be transferred to the predict script. The train-test classifier and regressor are set to use 4 cores. The script must be run on a machine that can support 4 cores. The final classifier and regressor in this script are set to use 1 core to provide the most cost efficient, rather than the fastest, model prediction. For information on generating inputs for this script or on setting up Google Cloud virtual machines, see the [project readme](https://github.com/accs-uaa/vegetation-cover-modeling).\n",
    "\n",
    "The distribution and abundance of a species is two distinct problems: 1. Where does the species occur? 2. Where the species occurs, how much is present? To represent the nested nature of the problems, we developed a hierarchical model where a classifier predicted species presence-absence and a regressor predicted species abundance in areas of predicted species presence. The practical advantage to this hierarchical method is that it accurately predicts absences, which are of disproportionate ecological value than exact prediction of other values along the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data and Variables\n",
    "\n",
    "This script relies on data that has been pre-processed into a csv file using the \"Format Taxon Data\" ArcGIS Pro script tool. The csv file must include all presence and absence observations for the target taxon or aggregate and all values for features extracted to the site locations. Features defined below must be modified to match the input csv file if changes to the construction of features are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root directory\n",
    "root_folder = 'N:/ACCS_Work'\n",
    "# Define data folders\n",
    "data_input = os.path.join(root_folder,\n",
    "                          'Projects/VegetationEcology/AKVEG_QuantitativeMap/Project_GIS/Data_Input/species_data')\n",
    "data_output = os.path.join(root_folder,\n",
    "                           'Projects/VegetationEcology/AKVEG_QuantitativeMap/Project_GIS/Data_Output/model_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Define input file\n",
    "input_file = os.path.join(data_input, 'mapClass_BetulaShrubs_Unrounded.csv')\n",
    "# Define output folder\n",
    "output_folder = os.path.join(data_output, 'betshr_r2')\n",
    "# Define output report\n",
    "output_report_name = 'betula-shrubs-report.html'\n",
    "# Define species, genera, or aggregate name\n",
    "taxon_name = 'Betula Shrubs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define variable sets\n",
    "predictor_all = ['aspect', 'wetness', 'elevation', 'slope', 'roughness', 'exposure', 'area', 'relief', 'position', 'radiation', 'sent2_05_11_shortIR1', 'sent2_05_12_shortIR2', 'sent2_05_2_blue', 'sent2_05_3_green', 'sent2_05_4_red', 'sent2_05_5_redge1', 'sent2_05_6_redge2', 'sent2_05_7_redge3', 'sent2_05_8_nearIR', 'sent2_05_8a_redge4', 'sent2_05_evi2', 'sent2_05_nbr', 'sent2_05_ndmi', 'sent2_05_ndsi', 'sent2_05_ndvi', 'sent2_05_ndwi', 'sent2_06_11_shortIR1', 'sent2_06_12_shortIR2', 'sent2_06_2_blue', 'sent2_06_3_green', 'sent2_06_4_red', 'sent2_06_5_redge1', 'sent2_06_6_redge2', 'sent2_06_7_redge3', 'sent2_06_8_nearIR', 'sent2_06_8a_redge4', 'sent2_06_evi2', 'sent2_06_nbr', 'sent2_06_ndmi', 'sent2_06_ndsi', 'sent2_06_ndvi', 'sent2_06_ndwi', 'sent2_07_11_shortIR1', 'sent2_07_12_shortIR2', 'sent2_07_2_blue', 'sent2_07_3_green', 'sent2_07_4_red', 'sent2_07_5_redge1', 'sent2_07_6_redge2', 'sent2_07_7_redge3', 'sent2_07_8_nearIR', 'sent2_07_8a_redge4', 'sent2_07_evi2', 'sent2_07_nbr', 'sent2_07_ndmi', 'sent2_07_ndsi', 'sent2_07_ndvi', 'sent2_07_ndwi', 'sent2_08_11_shortIR1', 'sent2_08_12_shortIR2', 'sent2_08_2_blue', 'sent2_08_3_green', 'sent2_08_4_red', 'sent2_08_5_redge1', 'sent2_08_6_redge2', 'sent2_08_7_redge3', 'sent2_08_8_nearIR', 'sent2_08_8a_redge4', 'sent2_08_evi2', 'sent2_08_nbr', 'sent2_08_ndmi', 'sent2_08_ndsi', 'sent2_08_ndvi', 'sent2_08_ndwi', 'sent2_09_11_shortIR1', 'sent2_09_12_shortIR2', 'sent2_09_2_blue', 'sent2_09_3_green', 'sent2_09_4_red', 'sent2_09_5_redge1', 'sent2_09_6_redge2', 'sent2_09_7_redge3', 'sent2_09_8_nearIR', 'sent2_09_8a_redge4', 'sent2_09_evi2', 'sent2_09_nbr', 'sent2_09_ndmi', 'sent2_09_ndsi', 'sent2_09_ndvi', 'sent2_09_ndwi']\n",
    "zero_variable = ['zero']\n",
    "cover = ['coverTotal']\n",
    "num_points = ['num_points']\n",
    "retain_variables = ['siteCode', 'day', 'year', 'nameAccepted', 'genus', 'initialProject', 'coverType', 'coverMethod', 'scopeVascular', 'scopeBryophyte', 'scopeLichen', 'plotDimensions', 'datum', 'latitude', 'longitude', 'error']\n",
    "all_variables = retain_variables + num_points + predictor_all + cover\n",
    "iteration = ['iteration']\n",
    "absence = ['absence']\n",
    "presence = ['presence']\n",
    "response = ['response']\n",
    "distribution = ['distribution']\n",
    "prediction = ['prediction']\n",
    "output_variables = all_variables + absence + presence + response + distribution + prediction + iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize\n",
    "\n",
    "This script has general dependencies on the *os* package for file system manipulations, the *numpy* and *pandas* packages for data manipulations, and the *seaborn* and *matplotlib* packages for plotting. *GPy* and *GPyOpt* are Gaussian Process packages that drive the bayesian optimization of hyperparameters. *XGBoost* provides the gradient boosting classifier and regressor used to create the composite predictions. *Scikit Learn* provides model selection, cross validation, performance tools. Joblib provides a function to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Import packages for file manipulation, data manipulation, and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "# Import packages for bayesian optimization\n",
    "import GPy\n",
    "import GPyOpt\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "# Import LightGBM gradient boosting implementations\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "# Import modules for model selection, cross validation, random forest, and performance from Scikit Learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "# Import joblib\n",
    "import joblib\n",
    "# Import timing packages\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a plots folder if it does not exist\n",
    "plots_folder = os.path.join(output_folder, \"plots\")\n",
    "if not os.path.exists(plots_folder):\n",
    "    os.makedirs(plots_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define output test data\n",
    "output_csv = os.path.join(output_folder, 'prediction.csv')\n",
    "# Define output model files\n",
    "output_classifier = os.path.join(output_folder, 'classifier.joblib')\n",
    "output_regressor = os.path.join(output_folder, 'regressor.joblib')\n",
    "# Define output threshold file\n",
    "threshold_file = os.path.join(output_folder, 'threshold.txt')\n",
    "# Define output correlation plot\n",
    "variable_correlation = os.path.join(plots_folder, \"variable_correlation.png\")\n",
    "# Define output variable importance plots\n",
    "importance_classifier = os.path.join(plots_folder, \"importance_classifier.png\")\n",
    "importance_regressor = os.path.join(plots_folder, \"importance_regressor.png\")\n",
    "# Define output bayesian optimization convergence plots\n",
    "convergence_classifier = os.path.join(plots_folder, \"convergence_classifier.png\")\n",
    "convergence_regressor = os.path.join(plots_folder, \"convergence_regressor.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions\n",
    "\n",
    "Analyses are conducted in units represented by functions. The functions are defined below in order of use. In general, functions in this script fall into three categories: Bayesian Optimization, Threshold Optimization, and Export Results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Bayesian Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define an optimization objective function for the xgboost classifier\n",
    "def cvClassifier(parameters):\n",
    "    # Define a 5-fold cross validation split method\n",
    "    cv_splits = KFold(n_splits=10, shuffle=False, random_state=314)\n",
    "    # Define the search parameter set\n",
    "    parameters = parameters[0]\n",
    "    # Define the cross validator\n",
    "    score = cross_val_score(\n",
    "        LGBMClassifier(boosting_type='gbdt',\n",
    "                       num_leaves=int(parameters[0]),\n",
    "                       max_depth=int(parameters[1]),\n",
    "                       learning_rate=parameters[2],\n",
    "                       n_estimators=500,\n",
    "                       subsample_for_bin=200000,\n",
    "                       objective='binary',\n",
    "                       class_weight=None,\n",
    "                       min_split_gain = parameters[3],\n",
    "                       min_child_weight = parameters[4],\n",
    "                       min_child_samples = int(parameters[5]),\n",
    "                       subsample = parameters[6],\n",
    "                       subsample_freq = 1,\n",
    "                       colsample_bytree = parameters[7],\n",
    "                       reg_alpha = parameters[8],\n",
    "                       reg_lambda = parameters[9],\n",
    "                       n_jobs = 16,\n",
    "                       silent = True,\n",
    "                       importance_type = 'gain'),\n",
    "        X_bayesian, y_bayesian, scoring='roc_auc', cv=cv_splits).mean()\n",
    "    # Convert the mean score to array and return the inverse of the array for minimization\n",
    "    score = np.array(score)\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define an optimization objective function for the xgboost regressor\n",
    "def cvRegressor(parameters):\n",
    "    # Define a 5-fold cross validation split method\n",
    "    cv_splits = KFold(n_splits=10, shuffle=False, random_state=314)\n",
    "    # Define the search parameter set\n",
    "    parameters = parameters[0]\n",
    "    # Define the cross validator\n",
    "    score = cross_val_score(\n",
    "        LGBMRegressor(boosting_type='gbdt',\n",
    "                      num_leaves=int(parameters[0]),\n",
    "                      max_depth=int(parameters[1]),\n",
    "                      learning_rate=parameters[2],\n",
    "                      n_estimators=500,\n",
    "                      subsample_for_bin=200000,\n",
    "                      objective='regression',\n",
    "                      class_weight=None,\n",
    "                      min_split_gain = parameters[3],\n",
    "                      min_child_weight = parameters[4],\n",
    "                      min_child_samples = int(parameters[5]),\n",
    "                      subsample = parameters[6],\n",
    "                      subsample_freq = 1,\n",
    "                      colsample_bytree = parameters[7],\n",
    "                      reg_alpha = parameters[8],\n",
    "                      reg_lambda = parameters[9],\n",
    "                      n_jobs = 16,\n",
    "                      silent = True,\n",
    "                      importance_type = 'gain'),\n",
    "        X_bayesian, y_bayesian, scoring='r2', cv=cv_splits).mean()\n",
    "    # Convert the mean score to array and return the inverse of the array for minimization\n",
    "    score = np.array(score)\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define an optimization function\n",
    "def bayesianOptimizer(objective_function, initial, iterations, plot_file):\n",
    "    # Create the hyperparameter search domain\n",
    "    domain=[{'name': 'num_leaves', 'type': 'discrete', 'domain': (7, 1000)},\n",
    "            {'name': 'max_depth', 'type': 'discrete', 'domain': (3, 100)},\n",
    "            {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.001, 0.2)},\n",
    "            {'name': 'min_split_gain', 'type': 'continuous', 'domain': (0, 0.1)},\n",
    "            {'name': 'min_child_weight', 'type': 'continuous', 'domain': (0, 1)},\n",
    "            {'name': 'min_child_samples', 'type': 'continuous', 'domain': (1, 200)},\n",
    "            {'name': 'subsample', 'type': 'continuous', 'domain': (.1, .9)},\n",
    "            {'name': 'colsample_bytree', 'type': 'continuous', 'domain': (.1, .9)},\n",
    "            {'name': 'reg_alpha', 'type': 'continuous', 'domain': (0, 5)},\n",
    "            {'name': 'reg_lambda', 'type': 'continuous', 'domain': (0, 5)},\n",
    "           ]\n",
    "    # Initialize the Bayesian Optimizer\n",
    "    optimizer = GPyOpt.methods.BayesianOptimization(f = objective_function,\n",
    "                                                    domain = domain,\n",
    "                                                    model_type = 'GP',\n",
    "                                                    initial_design_numdata = initial,\n",
    "                                                    initial_design_type = 'random',\n",
    "                                                    acquisition_type = 'EI',\n",
    "                                                    exact_feval=False,\n",
    "                                                    maximize=False)\n",
    "    # Run iterations of optimization\n",
    "    optimizer.run_optimization(max_iter=iterations)\n",
    "    # Plot convergence\n",
    "    optimizer.plot_convergence(filename=plot_file)\n",
    "    # Return results\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Threshold Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate performance metrics based on a specified threshold value\n",
    "def testPresenceThreshold(predict_probability, threshold, y_test):\n",
    "    # Create an empty array of zeroes that matches the length of the probability predictions\n",
    "    predict_thresholded = np.zeros(predict_probability.shape)\n",
    "    # Set values for all probabilities greater than or equal to the threshold equal to 1\n",
    "    predict_thresholded[predict_probability >= threshold] = 1\n",
    "    # Determine error rates\n",
    "    confusion_test = confusion_matrix(y_test, predict_thresholded)\n",
    "    true_negative = confusion_test[0,0]\n",
    "    false_negative = confusion_test[1,0]\n",
    "    true_positive = confusion_test[1,1]\n",
    "    false_positive = confusion_test[0,1]\n",
    "    # Calculate sensitivity and specificity\n",
    "    sensitivity = true_positive / (true_positive + false_negative)\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_test, predict_probability)\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = (true_negative + true_positive) / (true_negative + false_positive + false_negative + true_positive)\n",
    "    # Return the thresholded probabilities and the performance metrics\n",
    "    return (sensitivity, specificity, auc, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a function to determine a presence threshold\n",
    "def determineOptimalThreshold(predict_probability, y_test):\n",
    "    # Iterate through numbers between 0 and 1000 to output a list of sensitivity and specificity values per threshold number\n",
    "    i = 1\n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    while i < 1001:\n",
    "        threshold = i/1000\n",
    "        sensitivity, specificity, auc, accuracy = testPresenceThreshold(predict_probability, threshold, y_test)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "        i = i + 1\n",
    "    # Calculate a list of absolute value difference between sensitivity and specificity and find the optimal threshold\n",
    "    difference_list = [np.absolute(a - b) for a, b in zip(sensitivity_list, specificity_list)]\n",
    "    value, threshold = min((value, threshold) for (threshold, value) in enumerate(difference_list))\n",
    "    threshold = threshold/1000\n",
    "    # Calculate the performance of the optimal threshold\n",
    "    sensitivity, specificity, auc, accuracy = testPresenceThreshold(predict_probability, threshold, y_test)\n",
    "    # Return the optimal threshold and the performance metrics of the optimal threshold\n",
    "    return threshold, sensitivity, specificity, auc, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Export Results Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a function to composite model results\n",
    "def compositePrediction(test, presence, response, threshold):\n",
    "    # Define a function to threshold absences and set presences equal to regression response\n",
    "    def compositeRows(row):\n",
    "        if row[presence[0]] < threshold:\n",
    "            return 0\n",
    "        elif row[presence[0]] >= threshold:\n",
    "            return row[response[0]]\n",
    "    # Apply function to all rows in test data\n",
    "    test['prediction'] = test.apply(lambda row: compositeRows(row), axis=1)\n",
    "    # Return the test data frame with composited results\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to plot Pearson correlation of predictor variables\n",
    "def plotVariableCorrelation(X_train, outFile):\n",
    "    # Calculate Pearson correlation coefficient between the predictor variables, where -1 is perfect negative correlation and 1 is perfect positive correlation\n",
    "    correlation = X_train.astype('float64').corr()\n",
    "    # Generate a mask for the upper triangle of plot\n",
    "    mask = np.zeros_like(correlation, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plot.subplots(figsize=(20, 18))\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    correlation_plot = sns.heatmap(correlation, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={'shrink': .5})\n",
    "    correlation_figure = correlation_plot.get_figure()\n",
    "    correlation_figure.savefig(outFile, bbox_inches='tight', dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to plot variable importances\n",
    "def plotVariableImportances(inModel, X_train, outVariableFile):\n",
    "    # Get numerical feature importances\n",
    "    importances = list(inModel.feature_importances_)\n",
    "    # List of tuples with variable and importance\n",
    "    feature_list = list(X_train.columns)\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    # Initialize the plot and set figure size\n",
    "    variable_figure = plot.figure()\n",
    "    fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 20\n",
    "    fig_size[1] = 6\n",
    "    plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "    # Create list of x locations for plotting\n",
    "    x_values = list(range(len(importances)))\n",
    "    # Make a bar chart of the variable importances\n",
    "    plot.bar(x_values, importances, orientation = 'vertical')\n",
    "    # Tick labels for x axis\n",
    "    plot.xticks(x_values, feature_list, rotation='vertical')\n",
    "    # Axis labels and title\n",
    "    plot.ylabel('Importance'); plot.xlabel('Variable'); plot.title('Variable Importances');\n",
    "    # Export\n",
    "    variable_figure.savefig(outVariableFile, bbox_inches=\"tight\", dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conduct Analyses\n",
    "\n",
    "The analyses are subdivided into subsections: load data, assess untuned model performance, test bayesian optimization, train and test iterations with nested cross validation, final model training, and export results. The majority of analytical time is devoted to bayesian optimization within nested cross validation to ensure selection of a best performing model. Nested cross-validation is necessary to maintain the independence of the test data outside of cross-validated hyperparameter and threshold optimization. The XGBoost implementation of stochastic gradient boosting is the learning algorithm. The composited prediction results of the outer cross-validation, wherein each sample is predicted a single time, are appended into a single data frame for additional analyses and plotting external to this script.\n",
    "\n",
    "Outputs of the analysis are:\n",
    "1. Report of performance, including the following plots:\n",
    "  1. Final convergence of the classifier hyperparameters\n",
    "  2. Final convergence of the regressor hyperparameters\n",
    "  3. Final variable importances of the classifier\n",
    "  4. Final variable importances of the regressor\n",
    "  5. Pearson correlation for all predictors\n",
    "2. Model files:\n",
    "  1. Final Classifier\n",
    "  2. Final Regressor\n",
    "3. Report with overall R squared, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE) and absence AUC and accuracy.\n",
    "4. Final threshold\n",
    "5. Test predictions from 10-fold cross-validation (each sample predicted once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load Data\n",
    "\n",
    "Two data instances were created based on the input csv file: an input data frame and a presence only AIM NPR-A data subset. The presence-absence classifiers were trained from the input data and the abundance regressors were trained from the presence AIM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create data frame of input data\n",
    "input_data = pd.read_csv(input_file)\n",
    "# Convert values to floats\n",
    "input_data[predictor_all + cover] = input_data[predictor_all + cover].astype(float)\n",
    "# Convert values to integers\n",
    "input_data[zero_variable] = input_data[zero_variable].astype('int32')\n",
    "input_data[num_points] = input_data[num_points].astype('int32')\n",
    "# Remove inappropriate project datasets\n",
    "input_data = input_data[(input_data['initialProject'] != 'NPS CAKN Permafrost') &\n",
    "                       (input_data['initialProject'] != 'NPS Katmai LC') &\n",
    "                       (input_data['initialProject'] != 'NPS Yuch PA') &\n",
    "                       (input_data['initialProject'] != 'Shell ONES Remote Sensing') &\n",
    "                       (input_data['initialProject'] != 'USFWS IRM')]\n",
    "# Shuffle data\n",
    "input_data = shuffle(input_data, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Split the X data for classification\n",
    "X_classify = input_data[predictor_all]\n",
    "# Split the y data for classification\n",
    "y_classify = input_data[zero_variable[0]].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Subset the training data to cover and non-cover data\n",
    "cover_data = input_data[(input_data['coverType'] == 'Quantitative') |\n",
    "                        (input_data['coverType'] == 'Semi-quantitative')]\n",
    "cover_data = cover_data.reset_index()\n",
    "non_cover_data = input_data[(input_data['coverType'] != 'Quantitative') &\n",
    "                            (input_data['coverType'] != 'Semi-quantitative')]\n",
    "non_cover_data = non_cover_data.reset_index()\n",
    "# Subset the regression data\n",
    "regression_data = cover_data[cover_data['zero'] == 1]\n",
    "regression_data = regression_data[regression_data['num_points'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(regression_data['num_points']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Split the X and y data for regression\n",
    "X_regress = regression_data[predictor_all]\n",
    "y_regress = regression_data[cover[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 8\n",
    "fig_size[1] = 6\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "plot.style.use('grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Train and Test Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define 10-fold cross validation split methods\n",
    "outer_cv_splits = KFold(n_splits=10, shuffle=True, random_state=314)\n",
    "inner_cv_splits = KFold(n_splits=10, shuffle=False, random_state=314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create empty lists to store threshold and performance metrics\n",
    "threshold_list = []\n",
    "# Create an empty data frame to store the outer cross validation splits\n",
    "outer_train = pd.DataFrame(columns=all_variables + iteration)\n",
    "outer_test = pd.DataFrame(columns=all_variables + iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty data frame to store the outer test results\n",
    "outer_results = pd.DataFrame(columns=output_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create outer cross validation splits for cover data\n",
    "count = 1\n",
    "for train_index, test_index in outer_cv_splits.split(cover_data):\n",
    "    # Split the data into train and test partitions\n",
    "    train = cover_data.iloc[train_index]\n",
    "    test = cover_data.iloc[test_index]\n",
    "    # Insert iteration to train\n",
    "    train[iteration[0]] = count\n",
    "    # Insert iteration to test\n",
    "    test[iteration[0]] = count\n",
    "    # Append to data frames\n",
    "    outer_train = outer_train.append(train, ignore_index=True, sort=True)\n",
    "    outer_test = outer_test.append(test, ignore_index=True, sort=True)\n",
    "    # Increase counter\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create outer cross validation splits for non-cover data\n",
    "count = 1\n",
    "for train_index, test_index in outer_cv_splits.split(non_cover_data):\n",
    "    # Split the data into train and test partitions\n",
    "    train = non_cover_data.iloc[train_index]\n",
    "    test = non_cover_data.iloc[test_index]\n",
    "    # Insert iteration to train\n",
    "    train[iteration[0]] = count\n",
    "    # Insert iteration to test\n",
    "    test[iteration[0]] = count\n",
    "    # Append to data frames\n",
    "    outer_train = outer_train.append(train, ignore_index=True, sort=True)\n",
    "    outer_test = outer_test.append(test, ignore_index=True, sort=True)\n",
    "    # Increase counter\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Reset indices\n",
    "outer_train = outer_train.reset_index()\n",
    "outer_test = outer_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#### MODEL TRAIN AND TEST ITERATIONS WITH HYPERPARAMETER AND THRESHOLD OPTIMIZATION IN NESTED CROSS-VALIDATION\n",
    "####____________________________________________________\n",
    "\n",
    "# Iterate through outer cross validation splits\n",
    "i = 1\n",
    "while i < 11:\n",
    "    \n",
    "    \n",
    "    #### CONDUCT MODEL TRAIN\n",
    "    ####____________________________________________________\n",
    "    \n",
    "    # Partition the outer train split by iteration number\n",
    "    print(f'Conducting outer cross-validation iteration {i} of 10...')\n",
    "    train_iteration = outer_train[outer_train[iteration[0]] == i]\n",
    "    \n",
    "    # Identify X and y train splits for the classifier\n",
    "    X_train_classify = train_iteration[predictor_all]\n",
    "    y_train_classify = train_iteration[zero_variable[0]].astype('int32')\n",
    "    \n",
    "    # Set classifier convergence plot output\n",
    "    convergence_classifier_partial = os.path.splitext(convergence_classifier)[0] + str(i) + '.png'\n",
    "          \n",
    "    # Conduct Bayesian Optimization on the classifier train dataset using inner cross validation\n",
    "    print('\\tOptimizing classifier hyperparameters...')\n",
    "    iteration_start = time.time()\n",
    "    X_bayesian = X_train_classify\n",
    "    y_bayesian = y_train_classify\n",
    "    optimizer_classify = bayesianOptimizer(cvClassifier, 50, 100, convergence_classifier_partial)\n",
    "    # Select best set of parameters for the classifier\n",
    "    classifier_parameters = optimizer_classify.X[np.argmin(optimizer_classify.Y)]\n",
    "    iteration_end = time.time()\n",
    "    iteration_elapsed = int(iteration_end - iteration_start)\n",
    "    iteration_success_time = datetime.datetime.now()\n",
    "    print(f'\\tCompleted at {iteration_success_time.strftime(\"%Y-%m-%d %H:%M\")} (Elapsed time: {datetime.timedelta(seconds=iteration_elapsed)})')\n",
    "    print('\\t----------')\n",
    "          \n",
    "    # Create a classifier from optimized hyperparameters\n",
    "    classifier = LGBMClassifier(boosting_type='gbdt',\n",
    "                                num_leaves=int(classifier_parameters[0]),\n",
    "                                max_depth=int(classifier_parameters[1]),\n",
    "                                learning_rate=classifier_parameters[2],\n",
    "                                n_estimators=500,\n",
    "                                subsample_for_bin=200000,\n",
    "                                objective='binary',\n",
    "                                class_weight=None,\n",
    "                                min_split_gain = classifier_parameters[3],\n",
    "                                min_child_weight = classifier_parameters[4],\n",
    "                                min_child_samples = int(classifier_parameters[5]),\n",
    "                                subsample = classifier_parameters[6],\n",
    "                                subsample_freq = 1,\n",
    "                                colsample_bytree = classifier_parameters[7],\n",
    "                                reg_alpha = classifier_parameters[8],\n",
    "                                reg_lambda = classifier_parameters[9],\n",
    "                                n_jobs = 16,\n",
    "                                silent = True,\n",
    "                                importance_type = 'gain')\n",
    "    \n",
    "    # Predict each training data row in inner cross validation\n",
    "    print('\\tOptimizing classification threshold...')\n",
    "    iteration_start = time.time()\n",
    "          \n",
    "    # Create an empty data frame to store the inner cross validation splits\n",
    "    inner_train = pd.DataFrame(columns=all_variables + iteration + ['inner'])\n",
    "    inner_test = pd.DataFrame(columns=all_variables + iteration + ['inner'])\n",
    "          \n",
    "    # Create an empty data frame to store the inner test results\n",
    "    inner_results = pd.DataFrame(columns=all_variables + absence + presence + response + prediction + iteration + ['inner'])\n",
    "          \n",
    "    # Create inner cross validation splits\n",
    "    count = 1\n",
    "    for train_index, test_index in inner_cv_splits.split(train_iteration):\n",
    "        # Split the data into train and test partitions\n",
    "        train = train_iteration.iloc[train_index]\n",
    "        test = train_iteration.iloc[test_index]\n",
    "        # Insert iteration to train\n",
    "        train['inner'] = count\n",
    "        # Insert iteration to test\n",
    "        test['inner'] = count\n",
    "        # Append to data frames\n",
    "        inner_train = inner_train.append(train, ignore_index=True, sort=True)\n",
    "        inner_test = inner_test.append(test, ignore_index=True, sort=True)\n",
    "        # Increase counter\n",
    "        count += 1\n",
    "          \n",
    "    # Iterate through inner cross validation splits\n",
    "    n = 1\n",
    "    while n < 11:\n",
    "        inner_train_iteration = inner_train[inner_train['inner'] == n]\n",
    "        inner_test_iteration = inner_test[inner_test['inner'] == n]\n",
    "    \n",
    "        # Identify X and y inner train and test splits\n",
    "        X_train_inner = inner_train_iteration[predictor_all]\n",
    "        y_train_inner = inner_train_iteration[zero_variable[0]].astype('int32')\n",
    "        X_test_inner = inner_test_iteration[predictor_all]\n",
    "        y_test_inner = inner_test_iteration[zero_variable[0]].astype('int32')\n",
    "        \n",
    "        # Train classifier on the inner train data\n",
    "        classifier.fit(X_train_inner, y_train_inner)\n",
    "        \n",
    "        # Predict probabilities for inner test data\n",
    "        probability_inner = classifier.predict_proba(X_test_inner)\n",
    "        # Concatenate predicted values to test data frame\n",
    "        inner_test_iteration['absence'] = probability_inner[:,0]\n",
    "        inner_test_iteration['presence'] = probability_inner[:,1]\n",
    "          \n",
    "        # Add iteration number to inner test iteration\n",
    "        inner_test_iteration['inner'] = n\n",
    "    \n",
    "        # Add the test results to output data frame\n",
    "        inner_results = inner_results.append(inner_test_iteration, ignore_index=True, sort=True)\n",
    "        \n",
    "        # Increase n value\n",
    "        n += 1\n",
    "    \n",
    "    # Calculate the optimal threshold and performance of the presence-absence classification\n",
    "    inner_results[zero_variable[0]] = inner_results[zero_variable[0]].astype('int32')\n",
    "    threshold, sensitivity, specificity, auc, accuracy = determineOptimalThreshold(inner_results[presence[0]], inner_results[zero_variable[0]])\n",
    "    threshold_list.append(threshold)\n",
    "    iteration_end = time.time()\n",
    "    iteration_elapsed = int(iteration_end - iteration_start)\n",
    "    iteration_success_time = datetime.datetime.now()\n",
    "    print(f'\\tCompleted at {iteration_success_time.strftime(\"%Y-%m-%d %H:%M\")} (Elapsed time: {datetime.timedelta(seconds=iteration_elapsed)})')\n",
    "    print('\\t----------')\n",
    "    \n",
    "    # Train classifier\n",
    "    print('\\tTraining classifier...')\n",
    "    iteration_start = time.time()\n",
    "    classifier.fit(X_train_classify, y_train_classify)\n",
    "    iteration_end = time.time()\n",
    "    iteration_elapsed = int(iteration_end - iteration_start)\n",
    "    iteration_success_time = datetime.datetime.now()\n",
    "    print(f'\\tCompleted at {iteration_success_time.strftime(\"%Y-%m-%d %H:%M\")} (Elapsed time: {datetime.timedelta(seconds=iteration_elapsed)})')\n",
    "    print('\\t----------')\n",
    "    \n",
    "    # Partition the regressor train subset\n",
    "    regressor_subset = train_iteration[(train_iteration['coverType'] == 'Quantitative') |\n",
    "                                         (train_iteration['coverType'] == 'Semi-quantitative')]\n",
    "    regressor_subset = regressor_subset[regressor_subset['zero'] == 1]\n",
    "    regressor_subset = regressor_subset[regressor_subset['num_points'] == 1]\n",
    "    \n",
    "    # Identify X and y train splits for the regressor\n",
    "    X_train_regress = regressor_subset[predictor_all]\n",
    "    y_train_regress = regressor_subset[cover[0]]\n",
    "    \n",
    "    # Set regressor convergence plot output\n",
    "    convergence_regressor_partial = os.path.splitext(convergence_regressor)[0] + str(i) + '.png'\n",
    "    \n",
    "    # Conduct bayesian optimization of xgboost regressor\n",
    "    print('\\tOptimizing regressor hyperparameters...')\n",
    "    iteration_start = time.time()\n",
    "    X_bayesian = X_train_regress\n",
    "    y_bayesian = y_train_regress\n",
    "    optimizer_regress = bayesianOptimizer(cvRegressor, 50, 100, convergence_regressor_partial)\n",
    "    # Select best set of hyperparameters for the regressor\n",
    "    regressor_parameters = optimizer_regress.X[np.argmin(optimizer_regress.Y)]\n",
    "    iteration_end = time.time()\n",
    "    iteration_elapsed = int(iteration_end - iteration_start)\n",
    "    iteration_success_time = datetime.datetime.now()\n",
    "    print(f'\\tCompleted at {iteration_success_time.strftime(\"%Y-%m-%d %H:%M\")} (Elapsed time: {datetime.timedelta(seconds=iteration_elapsed)})')\n",
    "    print('\\t----------')\n",
    "    \n",
    "    # Create a regressor from optimized hyperparameters\n",
    "    regressor = LGBMRegressor(boosting_type='gbdt',\n",
    "                              num_leaves=int(regressor_parameters[0]),\n",
    "                              max_depth=int(regressor_parameters[1]),\n",
    "                              learning_rate=regressor_parameters[2],\n",
    "                              n_estimators=500,\n",
    "                              subsample_for_bin=200000,\n",
    "                              objective='regression',\n",
    "                              class_weight=None,\n",
    "                              min_split_gain = regressor_parameters[3],\n",
    "                              min_child_weight = regressor_parameters[4],\n",
    "                              min_child_samples = int(regressor_parameters[5]),\n",
    "                              subsample = regressor_parameters[6],\n",
    "                              subsample_freq = 1,\n",
    "                              colsample_bytree = regressor_parameters[7],\n",
    "                              reg_alpha = regressor_parameters[8],\n",
    "                              reg_lambda = regressor_parameters[9],\n",
    "                              n_jobs = 16,\n",
    "                              silent = True,\n",
    "                              importance_type = 'gain')\n",
    "    \n",
    "    # Train regressor\n",
    "    print('\\tTraining regressor...')\n",
    "    iteration_start = time.time()\n",
    "    regressor.fit(X_train_regress, y_train_regress)\n",
    "    iteration_end = time.time()\n",
    "    iteration_elapsed = int(iteration_end - iteration_start)\n",
    "    iteration_success_time = datetime.datetime.now()\n",
    "    print(f'\\tCompleted at {iteration_success_time.strftime(\"%Y-%m-%d %H:%M\")} (Elapsed time: {datetime.timedelta(seconds=iteration_elapsed)})')\n",
    "    print('\\t----------')\n",
    "          \n",
    "          \n",
    "    #### CONDUCT MODEL TEST\n",
    "    ####____________________________________________________\n",
    "    \n",
    "    # Partition the outer test split by iteration number\n",
    "    print('\\tPredicting outer cross-validation test data...')\n",
    "    iteration_start = time.time()\n",
    "    test_iteration = outer_test[outer_test[iteration[0]] == i]\n",
    "    \n",
    "    # Identify X test split\n",
    "    X_test = test_iteration[predictor_all]\n",
    "    \n",
    "    # Use the classifier to predict class probabilities\n",
    "    probability_prediction = classifier.predict_proba(X_test)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    test_iteration['absence'] = probability_prediction[:,0]\n",
    "    test_iteration['presence'] = probability_prediction[:,1]   \n",
    "    \n",
    "    # Convert probability to presence-absence\n",
    "    presence_zeros = np.zeros(test_iteration[presence[0]].shape)\n",
    "    presence_zeros[test_iteration[presence[0]] >= threshold] = 1\n",
    "    # Concatenate distribution values to test data frame\n",
    "    test_iteration['distribution'] = presence_zeros\n",
    "    \n",
    "    # Use the regressor to predict foliar cover response\n",
    "    response_prediction = regressor.predict(X_test)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    test_iteration['response'] = response_prediction\n",
    "    \n",
    "    # Composite the classifier and regressor predictions\n",
    "    test_iteration = compositePrediction(test_iteration, presence, response, threshold)\n",
    "    \n",
    "    # Add iteration number to test iteration\n",
    "    test_iteration[iteration[0]] = i\n",
    "    \n",
    "    # Add the test results to output data frame\n",
    "    outer_results = outer_results.append(test_iteration, ignore_index=True, sort=True)\n",
    "    iteration_end = time.time()\n",
    "    iteration_elapsed = int(iteration_end - iteration_start)\n",
    "    iteration_success_time = datetime.datetime.now()\n",
    "    print(f'\\tCompleted at {iteration_success_time.strftime(\"%Y-%m-%d %H:%M\")} (Elapsed time: {datetime.timedelta(seconds=iteration_elapsed)})')\n",
    "    print('\\t----------')\n",
    "    \n",
    "    # Increase iteration number\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(len(outer_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Partition output results to presence-absence observed and predicted\n",
    "y_classify_observed = outer_results[zero_variable[0]].astype('int32')\n",
    "y_classify_predicted = outer_results[distribution[0]].astype('int32')\n",
    "y_classify_probability = outer_results[presence[0]]\n",
    "\n",
    "# Determine error rates\n",
    "confusion_test = confusion_matrix(y_classify_observed, y_classify_predicted)\n",
    "true_negative = confusion_test[0,0]\n",
    "false_negative = confusion_test[1,0]\n",
    "true_positive = confusion_test[1,1]\n",
    "false_positive = confusion_test[0,1]\n",
    "# Calculate sensitivity and specificity\n",
    "sensitivity = true_positive / (true_positive + false_negative)\n",
    "specificity = true_negative / (true_negative + false_positive)\n",
    "# Calculate AUC score\n",
    "auc = roc_auc_score(y_classify_observed, y_classify_probability)\n",
    "# Calculate overall accuracy\n",
    "accuracy = (true_negative + true_positive) / (true_negative + false_positive + false_negative + true_positive)\n",
    "\n",
    "# Select regression-appropriate data from results\n",
    "cover_results = outer_results[(outer_results['coverType'] == 'Quantitative') |\n",
    "                              (outer_results['coverType'] == 'Semi-quantitative')]\n",
    "cover_results = cover_results[cover_results['num_points'] == 1]\n",
    "\n",
    "# Partition output results to foliar cover observed and predicted\n",
    "y_regress_observed = cover_results[cover[0]]\n",
    "y_regress_predicted = cover_results[prediction[0]]\n",
    "\n",
    "# Calculate performance metrics from output_results\n",
    "r_score = r2_score(y_regress_observed, y_regress_predicted, sample_weight=None, multioutput='uniform_average')\n",
    "mae = mean_absolute_error(y_regress_observed, y_regress_predicted)\n",
    "rmse = np.sqrt(mean_squared_error(y_regress_observed, y_regress_predicted))\n",
    "\n",
    "# Print performance results\n",
    "print(f'Final R^2 = {r_score}')\n",
    "print(f'Final MAE = {mae}')\n",
    "print(f'Final RMSE = {rmse}')\n",
    "print(f'Final AUC = {auc}')\n",
    "print(f'Final Accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Train Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 18\n",
    "fig_size[1] = 6\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#### TRAIN AND EXPORT FINAL CLASSIFIER\n",
    "\n",
    "# Conduct Bayesian Optimization on the classifier train dataset using inner cross validation\n",
    "X_bayesian = X_classify\n",
    "y_bayesian = y_classify\n",
    "optimizer_classify = bayesianOptimizer(cvClassifier, 50, 100, convergence_classifier)\n",
    "# Display highest AUC score achieved\n",
    "print(-np.amin(optimizer_classify.Y))\n",
    "# Select best set of parameters for the classifier\n",
    "classifier_parameters = optimizer_classify.X[np.argmin(optimizer_classify.Y)]\n",
    "    \n",
    "# Create a classifier from optimized hyperparameters\n",
    "classifier = LGBMClassifier(boosting_type='gbdt',\n",
    "                            num_leaves=int(classifier_parameters[0]),\n",
    "                            max_depth=int(classifier_parameters[1]),\n",
    "                            learning_rate=classifier_parameters[2],\n",
    "                            n_estimators=500,\n",
    "                            subsample_for_bin=200000,\n",
    "                            objective='binary',\n",
    "                            class_weight=None,\n",
    "                            min_split_gain = classifier_parameters[3],\n",
    "                            min_child_weight = classifier_parameters[4],\n",
    "                            min_child_samples = int(classifier_parameters[5]),\n",
    "                            subsample = classifier_parameters[6],\n",
    "                            subsample_freq = 1,\n",
    "                            colsample_bytree = classifier_parameters[7],\n",
    "                            reg_alpha = classifier_parameters[8],\n",
    "                            reg_lambda = classifier_parameters[9],\n",
    "                            n_jobs = 16,\n",
    "                            silent = True,\n",
    "                            importance_type = 'gain')\n",
    "\n",
    "# Create an empty data frame to store the inner cross validation splits\n",
    "inner_train = pd.DataFrame(columns=all_variables + iteration + ['inner'])\n",
    "inner_test = pd.DataFrame(columns=all_variables + iteration + ['inner'])\n",
    "          \n",
    "# Create an empty data frame to store the inner test results\n",
    "inner_results = pd.DataFrame(columns=all_variables + absence + presence + response + prediction + iteration + ['inner'])\n",
    "          \n",
    "# Create inner cross validation splits for input data\n",
    "count = 1\n",
    "for train_index, test_index in inner_cv_splits.split(input_data):\n",
    "    # Split the data into train and test partitions\n",
    "    train = input_data.iloc[train_index]\n",
    "    test = input_data.iloc[test_index]\n",
    "    # Insert iteration to train\n",
    "    train['inner'] = count\n",
    "    # Insert iteration to test\n",
    "    test['inner'] = count\n",
    "    # Append to data frames\n",
    "    inner_train = inner_train.append(train, ignore_index=True, sort=True)\n",
    "    inner_test = inner_test.append(test, ignore_index=True, sort=True)\n",
    "    # Increase counter\n",
    "    count += 1\n",
    "          \n",
    "# Iterate through inner cross validation splits\n",
    "n = 1\n",
    "while n < 11:\n",
    "    inner_train_iteration = inner_train[inner_train['inner'] == n]\n",
    "    inner_test_iteration = inner_test[inner_test['inner'] == n]\n",
    "    \n",
    "    # Identify X and y inner train and test splits\n",
    "    X_train_inner = inner_train_iteration[predictor_all]\n",
    "    y_train_inner = inner_train_iteration[zero_variable[0]].astype('int32')\n",
    "    X_test_inner = inner_test_iteration[predictor_all]\n",
    "    y_test_inner = inner_test_iteration[zero_variable[0]].astype('int32')\n",
    "        \n",
    "    # Train classifier on the inner train data\n",
    "    classifier.fit(X_train_inner, y_train_inner)\n",
    "        \n",
    "    # Predict probabilities for inner test data\n",
    "    probability_inner = classifier.predict_proba(X_test_inner)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    inner_test_iteration['absence'] = probability_inner[:,0]\n",
    "    inner_test_iteration['presence'] = probability_inner[:,1]\n",
    "          \n",
    "    # Add iteration number to inner test iteration\n",
    "    inner_test_iteration['inner'] = n\n",
    "    \n",
    "    # Add the test results to output data frame\n",
    "    inner_results = inner_results.append(inner_test_iteration, ignore_index=True, sort=True)\n",
    "    \n",
    "    # Increase n value\n",
    "    n += 1\n",
    "    \n",
    "# Calculate the optimal threshold and performance of the presence-absence classification\n",
    "inner_results[zero_variable[0]] = inner_results[zero_variable[0]].astype('int32')\n",
    "threshold_final, sensitivity, specificity, auc, accuracy = determineOptimalThreshold(inner_results[presence[0]], inner_results[zero_variable[0]])\n",
    "\n",
    "# Write a text file to store the presence-absence conversion threshold\n",
    "file = open(threshold_file, 'w')\n",
    "file.write(str(round(threshold_final, 5)))\n",
    "file.close()\n",
    "    \n",
    "# Train classifier\n",
    "classifier.fit(X_classify, y_classify)\n",
    "\n",
    "# Save classifier to an external file\n",
    "joblib.dump(classifier, output_classifier)\n",
    "# Export a variable importance plot\n",
    "plotVariableImportances(classifier, X_classify, importance_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN AND EXPORT A FINAL REGRESSOR\n",
    "\n",
    "# Conduct bayesian optimization of xgboost regressor\n",
    "X_bayesian = X_regress\n",
    "y_bayesian = y_regress\n",
    "optimizer_regress = bayesianOptimizer(cvRegressor, 50, 100, convergence_regressor)\n",
    "# Display highest R Squared score achieved\n",
    "print(-np.amin(optimizer_regress.Y))\n",
    "# Select best set of hyperparameters for the regressor\n",
    "regressor_parameters = optimizer_regress.X[np.argmin(optimizer_regress.Y)]\n",
    "    \n",
    "# Create a regressor from optimized hyperparameters\n",
    "regressor = LGBMRegressor(boosting_type='gbdt',\n",
    "                          num_leaves=int(regressor_parameters[0]),\n",
    "                          max_depth=int(regressor_parameters[1]),\n",
    "                          learning_rate=regressor_parameters[2],\n",
    "                          n_estimators=500,\n",
    "                          subsample_for_bin=200000,\n",
    "                          objective='regression',\n",
    "                          class_weight=None,\n",
    "                          min_split_gain = regressor_parameters[3],\n",
    "                          min_child_weight = regressor_parameters[4],\n",
    "                          min_child_samples = int(regressor_parameters[5]),\n",
    "                          subsample = regressor_parameters[6],\n",
    "                          subsample_freq = 1,\n",
    "                          colsample_bytree = regressor_parameters[7],\n",
    "                          reg_alpha = regressor_parameters[8],\n",
    "                          reg_lambda = regressor_parameters[9],\n",
    "                          n_jobs = 16,\n",
    "                          silent = True,\n",
    "                          importance_type = 'gain')\n",
    "\n",
    "# Train regressor\n",
    "regressor.fit(X_regress, y_regress)\n",
    "\n",
    "# Save classifier to an external file\n",
    "joblib.dump(regressor, output_regressor)\n",
    "# Export a variable importance plot\n",
    "plotVariableImportances(regressor, X_regress, importance_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Export test results to csv\n",
    "outer_results.to_csv(output_csv, header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 16\n",
    "fig_size[1] = 12\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Export a Pearson Correlation plot for the predictor variables\n",
    "plotVariableCorrelation(input_data[predictor_all], variable_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Write html text file\n",
    "output_report = os.path.join(output_folder, output_report_name)\n",
    "output_text = os.path.splitext(output_report)[0] + \".txt\"\n",
    "text_file = open(output_text, \"w\")\n",
    "text_file.write(f\"<html>\\n\")\n",
    "text_file.write(f\"<head>\\n\")\n",
    "text_file.write(f\"<meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\">\\n\")\n",
    "text_file.write(f\"<meta http-equiv=\\\"Expires\\\" content=\\\"-1\\\">\\n\")\n",
    "text_file.write(f\"</head>\\n\")\n",
    "text_file.write(f\"<body>\\n\")\n",
    "text_file.write(f\"<div style=\\\"width:90%;max-width:1000px;margin-left:auto;margin-right:auto\\\">\\n\")\n",
    "text_file.write(f\"<h1 style=\\\"text-align:center;\\\">Distribution-abundance model performance for \" + taxon_name + \"</h1>\\n\")\n",
    "text_file.write(f\"<br>\" + \"\\n\")\n",
    "text_file.write(f\"<h2>Predicted Distribution-abundance Pattern</h2>\\n\")\n",
    "text_file.write(f\"<p>Distribution-abundance was predicted by a composite hierarchical model: 1. a classifier predicted species presence or absence, and 2. a regressor predicted species foliar cover in areas where the classifier predicted the species to be present. The map below shows the output raster prediction.</p>\\n\")\n",
    "text_file.write(f\"<p><i>Prediction step has not yet been performed. No output to display.</i></p>\\n\")\n",
    "text_file.write(f\"<h2>Bayesian Optimization of Hyperparameters</h2>\\n\")\n",
    "text_file.write(f\"<p>The hyperparameters of the classifier and regressor were independently optimized in a bayesian framework using a Gaussian Process as the generative model. Optimization performance was determined by 5-fold cross validation using area under the receiver operating characteristic curve (AUC) as the metric to maximize for the classifier and R squared as the metric to maximize for the regressor. 250 optimization iterations were performed for the classifier and 500 were performed for the regressor. The best set of parameters was selected based on the maximization criteria.</p>\\n\")\n",
    "text_file.write(f\"<h3>Classifier Optimization</h3>\\n\")\n",
    "text_file.write(f\"<p>The hyperparameters of the gradient boosting classifier (using the LightGBM implementation) were optimized to the following values:</p>\\n\")\n",
    "text_file.write(f\"<p>boosting_type = 'gbdt'</p>\\n\")\n",
    "text_file.write(f\"<p>num_leaves = {str(int(classifier_parameters[0]))}</p>\\n\")\n",
    "text_file.write(f\"<p>max_depth = {str(int(classifier_parameters[1]))}</p>\\n\")\n",
    "text_file.write(f\"<p>learning_rate = {str(classifier_parameters[2])}</p>\\n\")\n",
    "text_file.write(f\"<p>n_estimators = 500</p>\\n\")\n",
    "text_file.write(f\"<p>subsample_for_bin = 200000</p>\\n\")\n",
    "text_file.write(f\"<p>objective = 'binary'</p>\\n\")\n",
    "text_file.write(f\"<p>class_weight = None</p>\\n\")\n",
    "text_file.write(f\"<p>min_split_gain = {str(classifier_parameters[3])}</p>\\n\")\n",
    "text_file.write(f\"<p>min_child_weight = {str(classifier_parameters[4])}</p>\\n\")\n",
    "text_file.write(f\"<p>min_child_samples = {str(int(classifier_parameters[5]))}</p>\\n\")\n",
    "text_file.write(f\"<p>subsample = {str(classifier_parameters[6])}</p>\\n\")\n",
    "text_file.write(f\"<p>subsample_freq = 1</p>\\n\")\n",
    "text_file.write(f\"<p>colsample_bytree = {str(classifier_parameters[7])}</p>\\n\")\n",
    "text_file.write(f\"<p>reg_alpha = {str(classifier_parameters[8])}</p>\\n\")\n",
    "text_file.write(f\"<p>reg_lambda = {str(classifier_parameters[9])}</p>\\n\")\n",
    "text_file.write(f\"<p>n_estimators = 16</p>\\n\")\n",
    "text_file.write(f\"<a target='_blank' href='plots\\\\convergence_classifier.png'><img style='display:inline-block;max-width:720px;width:100%;' src='plots\\\\convergence_classifier.png'></a>\\n\")\n",
    "text_file.write(f\"<h3>Regressor Optimization</h3>\\n\")\n",
    "text_file.write(f\"<p>The hyperparameters of the gradient boosting regressor (using the XGBoost implementation) were optimized to the following values:</p>\\n\")\n",
    "text_file.write(f\"<p>boosting_type = 'gbdt'</p>\\n\")\n",
    "text_file.write(f\"<p>num_leaves = {str(int(regressor_parameters[0]))}</p>\\n\")\n",
    "text_file.write(f\"<p>max_depth = {str(int(regressor_parameters[1]))}</p>\\n\")\n",
    "text_file.write(f\"<p>learning_rate = {str(regressor_parameters[2])}</p>\\n\")\n",
    "text_file.write(f\"<p>n_estimators = 500</p>\\n\")\n",
    "text_file.write(f\"<p>subsample_for_bin = 200000</p>\\n\")\n",
    "text_file.write(f\"<p>objective = 'regression'</p>\\n\")\n",
    "text_file.write(f\"<p>class_weight = None</p>\\n\")\n",
    "text_file.write(f\"<p>min_split_gain = {str(regressor_parameters[3])}</p>\\n\")\n",
    "text_file.write(f\"<p>min_child_weight = {str(regressor_parameters[4])}</p>\\n\")\n",
    "text_file.write(f\"<p>min_child_samples = {str(int(regressor_parameters[5]))}</p>\\n\")\n",
    "text_file.write(f\"<p>subsample = {str(regressor_parameters[6])}</p>\\n\")\n",
    "text_file.write(f\"<p>subsample_freq = 1</p>\\n\")\n",
    "text_file.write(f\"<p>colsample_bytree = {str(regressor_parameters[7])}</p>\\n\")\n",
    "text_file.write(f\"<p>reg_alpha = {str(regressor_parameters[8])}</p>\\n\")\n",
    "text_file.write(f\"<p>reg_lambda = {str(regressor_parameters[9])}</p>\\n\")\n",
    "text_file.write(f\"<p>n_estimators = 16</p>\\n\")\n",
    "text_file.write(f\"<a target='_blank' href='plots\\\\convergence_regressor.png'><img style='display:inline-block;max-width:720px;width:100%;' src='plots\\\\convergence_regressor.png'></a>\\n\")\n",
    "text_file.write(f\"<h2>Composite Model Performance</h2>\\n\")\n",
    "text_file.write(f\"<p>Model performance was measured by calculating R squared, mean absolute error, and root mean squared error for the composite prediction across five independent cross validation folds. Within each cross validation fold, the Bayesian Optimization and threshold calculation were nested with 5-fold cross validation. Nested cross-validation maintained the independence of the test partition through multiple rounds of cross-validated model optimization. Additionally, the performance of the absence class is reported as an area under the receiver operating characteristic curve (AUC) and overall accuracy, where specificity and sensitivity are as close to equal as possible (i.e, the model performs equally well at predicting absences and presences). All performance results are reported from the merged results of all cross-validation runs (i.e., the performance is based on inclusion of each sample in the test set once).</p>\\n\")\n",
    "text_file.write(f\"<h3>Overall Performance</h3>\\n\")\n",
    "text_file.write(f\"<p>R Squared = \" + str(np.round(r_score, 2)) + \"</p>\\n\")\n",
    "text_file.write(f\"<p>Mean Absolute Error = \" + str(np.round(mae, 2)) + \"</p>\\n\")\n",
    "text_file.write(f\"<p>Root Mean Squared Error = \" + str(np.round(rmse, 2)) + \"</p>\\n\")\n",
    "text_file.write(f\"<h3>Absence Performance</h3>\\n\")\n",
    "text_file.write(f\"<p>AUC = \" + str(np.round(auc, 2)) + \"</p>\\n\")\n",
    "text_file.write(f\"<p>Presence-Absence Accuracy = \" + str(np.round(accuracy, 2)) + \"</p>\\n\")\n",
    "text_file.write(f\"<h3>Classifier Importances</h3>\\n\")\n",
    "text_file.write(f\"<p>The Variable Importance plot for the classifier is shown below:</p>\\n\")\n",
    "text_file.write(f\"<a target='_blank' href='plots\\\\importance_classifier.png'><img style='display:inline-block;max-width:1000px;width:100%;' src='plots\\\\importance_classifier.png'></a>\\n\")\n",
    "text_file.write(f\"<h3>Regressor Importances</h3>\\n\")\n",
    "text_file.write(f\"<p>The Variable Importance plot for the regressor is shown below:</p>\\n\")\n",
    "text_file.write(f\"<a target='_blank' href='plots\\\\importance_regressor.png'><img style='display:inline-block;max-width:1000px;width:100%;' src='plots\\\\importance_regressor.png'></a>\\n\")\n",
    "text_file.write(fr\"<h2>Variable Correlation</h2>\" + \"\\n\")\n",
    "text_file.write(f\"<p>The plot below explores variable correlation. No attempt was made to remove highly correlated variables (shown in the plot dark blue).</p>\\n\")\n",
    "text_file.write(f\"<a target='_blank' href='plots\\\\variable_correlation.png'><img style='display:inline-block;width:100%;' src='plots\\\\variable_correlation.png'></a>\\n\")\n",
    "text_file.write(f\"</div>\\n\")\n",
    "text_file.write(f\"</body>\\n\")\n",
    "text_file.write(f\"</html>\\n\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Rename HTML Text to HTML\n",
    "if os.path.exists(output_report) == True:\n",
    "    os.remove(output_report)\n",
    "os.rename(output_text, output_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
