{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ec826c-c2f9-4ddb-a0ca-b1765fbe178b",
   "metadata": {},
   "source": [
    "# EVWHS Raw NTF Strips to MS 4-band 2m ortho TOA reflectance\n",
    "1. Clone PGC imagery_utils repo\n",
    "1. Edit lib/ortho_functions.py omax setting for Int16 reflectance 'rf' mode: VNIR (from 2000.0 to 10000.0) and SWIR/CAVIS (16000.0 to 10000.0)\n",
    "1. Create and activate PGC conda env\n",
    "1. Create list of multispectral NTF files\n",
    "1. Generate 4-band TOA orthos for each sub-image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86a98575-6ed8-4e31-bf37-63fd19c5c82d",
   "metadata": {},
   "source": [
    "conda create --name pgc -c conda-forge git python=3.11 gdal=3.6.4 globus-sdk globus-cli numpy scipy pandas geopandas rasterio shapely postgresql psycopg2 sqlalchemy configargparse lxml pathlib2 python-dateutil pytest rtree xlsxwriter tqdm alive-progress pyperclip --yes\n",
    "\n",
    "conda activate pgc\n",
    "cd /data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/unzipped\n",
    "\n",
    "find /data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/unzipped -type d -name \"*_MUL\" -exec find {} -type f -name \"*.NTF\" \\; > mul_ntf_files_order20250221.txt\n",
    "\n",
    "#edited lib/ortho_utils.py to set omax for Int16 reflectance to 10000 instead of 2000 (16000 for SWIR/CAVIS)\n",
    "\n",
    "python /home/mmacander/programming/abr/imagery_utils/pgc_ortho.py --bgrn --resample cubic --tap --pyramid-type cubic --threads 12 --epsg 3338 -r 2 -c rf  -f GTiff -t UInt16 -d /data/gis/gis_base/DEM/ifsar/wgs1984_ellipsoid_height/alaska_ifsar_dsm_20200925_plus_us_noaa_g2009.tif /data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/unzipped/mul_ntf_files_order20250221.txt /data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968ab65-a936-4cf8-8a92-81f9d18e8a38",
   "metadata": {},
   "source": [
    "1. Choose latest gdal env for notebook (cog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b280a331-e40a-494f-9585-3c22001c8be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from osgeo import gdal\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "from platform import python_version\n",
    "print(python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef809dc-4434-43f7-abd5-750f57fc454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running: /home/mmacander/miniconda3/bin/conda run -n cog gdalwarp --version\n",
      "GDAL 3.10.1, released 2025/01/08\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/home/mmacander/miniconda3/bin/conda', 'run', '-n', 'cog', 'gdalwarp', '--version'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test running subprocess gdal in a specific conda env\n",
    "import subprocess\n",
    "\n",
    "conda_path = \"/home/mmacander/miniconda3/bin/conda\"\n",
    "conda_env = \"cog\"  # Change this to your environment name\n",
    "\n",
    "# subprocess.run([\"conda\", \"run\", \"-n\", conda_env, \"gdalwarp\", \"--version\"], check=True)\n",
    "\n",
    "warp_cmd = [\n",
    "    conda_path, \"run\", \"-n\", conda_env,\n",
    "    \"gdalwarp\",\n",
    "    \"--version\"]\n",
    "\n",
    "# Print and execute the command\n",
    "print(f\"üöÄ Running: {' '.join(warp_cmd)}\")\n",
    "subprocess.run(warp_cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40527c9e-2967-403b-a71e-c42396d82c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210509-M1BS-050296456010_01_P008_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21205720-M1BS-050296474010_01_P010_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/21AUG30204810-M1BS-050296461010_01_P005_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL26212327-M1BS-050296496010_01_P005_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/24JUN29212914-M1BS-050296500010_01_P008_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210510-M1BS-050296456010_01_P007_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/24JUN29212846-M1BS-050296485010_01_P001_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/13JUN18214810-M1BS-050296499010_01_P002_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/13JUL27210947-M1BS-050296487010_01_P008_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23AUG23205624-M1BS-050296457010_01_P003_u16rf3338.tif']\n"
     ]
    }
   ],
   "source": [
    "ortho_subimage_dir = \"/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc\"\n",
    "os.chdir(ortho_subimage_dir)\n",
    "\n",
    "output_dir = \"/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_toa_strips\"\n",
    "pathlib.Path(os.path.join(output_dir, \"vrt\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "subimages = glob.glob(ortho_subimage_dir + \"/*.tif\")\n",
    "print(subimages[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1b4897-b357-4b8a-8b8c-a9628de4a77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          order_id                                         input_path\n",
      "0  050296456010_01  [/data/gis/raster_base/Alaska/AKVegMap/EVWHS/m...\n",
      "1  050296457010_01  [/data/gis/raster_base/Alaska/AKVegMap/EVWHS/m...\n",
      "2  050296458010_01  [/data/gis/raster_base/Alaska/AKVegMap/EVWHS/m...\n",
      "3  050296459010_01  [/data/gis/raster_base/Alaska/AKVegMap/EVWHS/m...\n",
      "4  050296460010_01  [/data/gis/raster_base/Alaska/AKVegMap/EVWHS/m...\n"
     ]
    }
   ],
   "source": [
    "# Extract filename from path and split on dashes/underscores\n",
    "data = []\n",
    "for path in subimages:\n",
    "    filename = os.path.basename(path)  # Get file name from path\n",
    "    name_parts = [p for part in filename.split('_') for p in part.split('-')]  # Split on _ and -\n",
    "    order_id = f\"{name_parts[2]}_{name_parts[3]}\"\n",
    "    data.append([path] + [filename] + [order_id])  # Store filename + split parts\n",
    "\n",
    "# Create DataFrame with dynamic column names\n",
    "# max_splits = max(len(row) for row in data)\n",
    "# print(max_splits)\n",
    "\n",
    "columns = [\"input_path\"] + [\"input_file\"] + [\"order_id\"]# [f\"Part_{i+1}\" for i in range(max_splits - 2)]\n",
    "# print(data)\n",
    "# print(columns)\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# print(df)\n",
    "\n",
    "# Group by order_id and aggregate input_path as a list\n",
    "order_grouped_df = df.groupby(\"order_id\")[\"input_path\"].agg(list).reset_index()\n",
    "print(order_grouped_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424433d5-49bf-42c4-8dac-2eb2ae2b9ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 9 NTF files to process.\n",
      "['/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210509-M1BS-050296456010_01_P008_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210510-M1BS-050296456010_01_P007_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210513-M1BS-050296456010_01_P004_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210517-M1BS-050296456010_01_P001_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210511-M1BS-050296456010_01_P006_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210512-M1BS-050296456010_01_P005_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210508-M1BS-050296456010_01_P009_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210515-M1BS-050296456010_01_P003_u16rf3338.tif', '/data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210516-M1BS-050296456010_01_P002_u16rf3338.tif']\n",
      "\n",
      " /data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_pgc/23JUL21210509-M1BS-050296456010_01_P008_u16rf3338.tif\n"
     ]
    }
   ],
   "source": [
    "# Define search pattern to find all .ntf files in subdirectories\n",
    "# ntf_files = glob.glob(order_dir + \"*_MUL/*.NTF\", recursive=True)\n",
    "ntf_files = order_grouped_df.iloc[0][\"input_path\"]\n",
    "\n",
    "# ntf_files = glob.glob(\"**/*.ntf\", recursive=True)\n",
    "# output_cog = \"/data/gis/raster_base/Alaska/AKVegMap/EVWHS/nome_beaver/outputs_python/final_mosaic_cog4.tif \"  # Output COG filename\n",
    "\n",
    "# Ensure we found NTF files\n",
    "if not ntf_files:\n",
    "    raise ValueError(\"‚ùå No .ntf files found in subdirectories!\")\n",
    "\n",
    "print(f\"‚úÖ Found {len(ntf_files)} NTF files to process.\")\n",
    "print(ntf_files)\n",
    "\n",
    "ntf_file = ntf_files[0]\n",
    "print(\"\\n\", ntf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead20257-5431-44ae-903d-e5ae8ad34057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NTF Metadata:\n",
      "NITF_ABPP: 11\n",
      "NITF_CCS_COLUMN: 0\n",
      "NITF_CCS_ROW: 0\n",
      "NITF_CLEVEL: 06\n",
      "NITF_ENCRYP: 0\n",
      "NITF_FBKGC: 126,126,126\n",
      "NITF_FDT: 20250221024806\n",
      "NITF_FHDR: NITF02.10\n",
      "NITF_FSCLAS: U\n",
      "NITF_FSCLSY: US\n",
      "NITF_FSCOP: 00000\n",
      "NITF_FSCPYS: 00000\n",
      "NITF_FTITLE: 23JUL21210509-M1BS-050296456010_01_P008.NTF\n",
      "NITF_IALVL: 0\n",
      "NITF_IC: NC\n",
      "NITF_ICAT: MS\n",
      "NITF_ICORDS: G\n",
      "NITF_IDATIM: 20230721210510\n",
      "NITF_IDLVL: 1\n",
      "NITF_IGEOLO: 631139N1443650W631311N1441302W630447N1441258W630247N1443726W\n",
      "NITF_IID1: M1EA9D8000\n",
      "NITF_IID2: 21JUL23WV021400023JUL21210509-M1BS-050296456010_01_P008\n",
      "NITF_ILOC_COLUMN: 0\n",
      "NITF_ILOC_ROW: 0\n",
      "NITF_IMAG: 1.0 \n",
      "NITF_IMAGE_COMMENTS: The imagery and metadata data on this media is the property of                  Maxar Technologies and is licensed for use only.                                All use must be in accordance with the terms of the license that                accompanies the media. If the license is purchased under contract               use is in accordance with the license therein                                   \n",
      "NITF_IMODE: S\n",
      "NITF_IREP: MULTI\n",
      "NITF_ISCLAS: U\n",
      "NITF_ISCLSY: US\n",
      "NITF_ISORCE: WV02\n",
      "NITF_ONAME: Maxar\n",
      "NITF_OPHONE: +1(800)496-1225\n",
      "NITF_OSTAID: Maxar\n",
      "NITF_PIAIMC_CLOUDCVR: 001\n",
      "NITF_PIAIMC_COMGEN: 00\n",
      "NITF_PIAIMC_ESD: Y\n",
      "NITF_PIAIMC_GENERATION: 1\n",
      "NITF_PIAIMC_IDATUM: WGE\n",
      "NITF_PIAIMC_MEANGSD: 00094.3\n",
      "NITF_PIAIMC_PREPROC: 1R\n",
      "NITF_PIAIMC_SATTRACK: 00000000\n",
      "NITF_PIAIMC_SENSMODE: PUSHBROOM\n",
      "NITF_PIAIMC_SENSNAME: WV02\n",
      "NITF_PIAIMC_SOURCE: VNIR: 10300100EA9D8000\n",
      "NITF_PIAIMC_SRP: Y\n",
      "NITF_PJUST: R\n",
      "NITF_PVTYPE: INT\n",
      "NITF_STDIDC_ACQUISITION_DATE: 20230721210510\n",
      "NITF_STDIDC_END_COLUMN: 009\n",
      "NITF_STDIDC_END_ROW: 00007\n",
      "NITF_STDIDC_END_SEGMENT: AA\n",
      "NITF_STDIDC_LOCATION: 6312N14426W\n",
      "NITF_STDIDC_MISSION: WV02\n",
      "NITF_STDIDC_OP_NUM: 000\n",
      "NITF_STDIDC_PASS: 14\n",
      "NITF_STDIDC_REPLAY_REGEN: 000\n",
      "NITF_STDIDC_REPRO_NUM: 00\n",
      "NITF_STDIDC_START_COLUMN: 001\n",
      "NITF_STDIDC_START_ROW: 00001\n",
      "NITF_STDIDC_START_SEGMENT: AA\n",
      "NITF_STYPE: BF01\n",
      "NITF_USE00A_ANGLE_TO_NORTH: 090\n",
      "NITF_USE00A_DYNAMIC_RANGE: 02046\n",
      "NITF_USE00A_MAX_LP_SEG: 006504\n",
      "NITF_USE00A_MEAN_GSD: 094.3\n",
      "NITF_USE00A_N_REF: 00\n",
      "NITF_USE00A_N_SEG: 001\n",
      "NITF_USE00A_OBL_ANG: 28.79\n",
      "NITF_USE00A_REV_NUM: 72328\n",
      "NITF_USE00A_ROLL_ANG: -18.95\n",
      "NITF_USE00A_SUN_AZ: 166.6\n",
      "NITF_USE00A_SUN_EL: +46.9\n",
      "AREA_OR_POINT: Area\n",
      "üìå Number of bands: 4\n",
      "üìå Geotransform: (469718.0, 2.0, 0.0, 1508406.0, 0.0, -2.0)\n",
      "üìå Projection: PROJCS[\"unknown\",GEOGCS[\"NAD83\",DATUM[\"North American Datum 1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101004]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",50],PARAMETER[\"longitude_of_center\",-154],PARAMETER[\"standard_parallel_1\",55],PARAMETER[\"standard_parallel_2\",65],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
      "üìå Dimensions: 10822 x 11056 pixels\n",
      "üìå Band 1: Type=UInt16, NoDataValue=65535.0\n",
      "üìå Band 2: Type=UInt16, NoDataValue=65535.0\n",
      "üìå Band 3: Type=UInt16, NoDataValue=65535.0\n",
      "üìå Band 4: Type=UInt16, NoDataValue=65535.0\n",
      "WV02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmacander/miniconda3/envs/cog/lib/python3.9/site-packages/osgeo/gdal.py:311: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "# Examine one file\n",
    "\n",
    "# Open the NTF file\n",
    "dataset = gdal.Open(ntf_file)\n",
    "if not dataset:\n",
    "    raise ValueError(f\"‚ùå Could not open {ntf_file}\")\n",
    "\n",
    "# Get general metadata\n",
    "metadata = dataset.GetMetadata()\n",
    "print(\"‚úÖ NTF Metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Get the number of bands\n",
    "num_bands = dataset.RasterCount\n",
    "print(f\"üìå Number of bands: {num_bands}\")\n",
    "\n",
    "# Get geotransform (spatial metadata)\n",
    "geotransform = dataset.GetGeoTransform()\n",
    "print(f\"üìå Geotransform: {geotransform}\")\n",
    "\n",
    "# Get projection (Coordinate Reference System)\n",
    "projection = dataset.GetProjection()\n",
    "print(f\"üìå Projection: {projection}\")\n",
    "\n",
    "# Get dimensions\n",
    "width = dataset.RasterXSize\n",
    "height = dataset.RasterYSize\n",
    "print(f\"üìå Dimensions: {width} x {height} pixels\")\n",
    "\n",
    "# Get information about each band\n",
    "for i in range(1, num_bands + 1):\n",
    "    band = dataset.GetRasterBand(i)\n",
    "    print(f\"üìå Band {i}: Type={gdal.GetDataTypeName(band.DataType)}, NoDataValue={band.GetNoDataValue()}\")\n",
    "\n",
    "# Close the dataset\n",
    "dataset = None\n",
    "\n",
    "NITF_ISORCE = metadata['NITF_ISORCE']\n",
    "print(NITF_ISORCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a4b0afe-7892-40ff-a085-9106653a1bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 9 NTF files.\n",
      "10300100EA9D8000\n",
      "10300100EA9D8000\n",
      "10300100EA9D8000\n",
      "10300100EA9D8000\n",
      "10300100EA9D8000\n",
      "10300100EA9D8000\n",
      "10300100EA9D8000\n",
      "10300100EA9D8000\n",
      "10300100EA9D8000\n"
     ]
    }
   ],
   "source": [
    "# Test extraction of catalog id\n",
    "# Ensure we have NTF files\n",
    "if not ntf_files:\n",
    "    raise ValueError(\"‚ùå No .ntf files found!\")\n",
    "\n",
    "print(f\"‚úÖ Found {len(ntf_files)} NTF files.\")\n",
    "\n",
    "# Loop through each NTF file to extract the `NITF_PIAIMC_SOURCE` metadata\n",
    "for ntf_file in ntf_files:\n",
    "    dataset = gdal.Open(ntf_file)\n",
    "    if dataset:\n",
    "        metadata = dataset.GetMetadata()\n",
    "        source_metadata = metadata.get(\"NITF_PIAIMC_SOURCE\", \"Unknown\")\n",
    "        \n",
    "        # Check if the source contains \"VNIR\"\n",
    "        # if \"VNIR\" in source_info:\n",
    "        #     print(f\"üìå {ntf_file}: {source_info}\")\n",
    "        # vnir_str = source_info.get(\"VNIR\", \"VNIR key not found\")\n",
    "    # Check if VNIR exists inside the metadata string\n",
    "    if \"VNIR:\" in source_metadata:\n",
    "        for line in source_metadata.split(\"\\n\"):  # Handle multi-line metadata\n",
    "            if line.startswith(\"VNIR:\"):\n",
    "                vnir_value = line.split(\":\", 1)[1].strip()  # Get value after colon\n",
    "       \n",
    "        print(vnir_value)\n",
    "        # return(vnir_value)\n",
    "        dataset = None  # Close dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "125d0dda-7fc1-4a45-9466-d6178547cb71",
   "metadata": {},
   "source": [
    "# Run on batch of unzipped order_dirs\n",
    "# print(order_grouped_df)\n",
    "for index, row in order_grouped_df.iterrows():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8e299-514c-4948-8b58-38a2c407a999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id                                        050296456010_01\n",
      "input_path    [/data/gis/raster_base/Alaska/AKVegMap/EVWHS/m...\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Order id: 050296456010_01\n",
      "‚úÖ Found 9 NTF files.\n",
      "10300100EA9D8000\n",
      "‚úÖ Earliest acquisition date: 20230721_210509\n",
      "Catalog ID: 10300100EA9D8000\n",
      "Output COG Filename: /data/gis/raster_base/Alaska/AKVegMap/EVWHS/mentasta/ortho_toa_strips/MS4_2m_20230721_210509_WV02_10300100EA9D8000_v20250228102438.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "ERROR 1: PROJ: proj_create: no database context specified\n",
      "ERROR 1: Cannot parse CRS http://www.opengis.net/def/crs/EPSG/0/5482\n",
      "ERROR 1: PROJ: proj_create: no database context specified\n",
      "ERROR 1: Cannot parse CRS http://www.opengis.net/def/crs/EPSG/0/5936\n",
      "ERROR 1: PROJ: proj_create: no database context specified\n",
      "ERROR 1: Cannot parse CRS http://www.opengis.net/def/crs/EPSG/0/3978\n",
      "ERROR 1: PROJ: proj_create: no database context specified\n",
      "ERROR 1: Cannot parse CRS http://www.opengis.net/def/crs/EPSG/0/2193\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n",
      "ERROR 1: PROJ: proj_create_from_name: Open of /home/mmacander/miniconda3/envs/cog/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "# Run on batch of images from one order\n",
    "for index, row in order_grouped_df.iterrows():\n",
    "    print(row)\n",
    "    print(f\"\\nOrder id: {row['order_id']}\")\n",
    "    # Define search pattern to find all .ntf files in subdirectories\n",
    "    # ntf_files = glob.glob(order_dir + \"*_MUL/*.NTF\", recursive=True)\n",
    "    ntf_files = row[\"input_path\"]\n",
    "    \n",
    "    # Ensure we have files\n",
    "    if not ntf_files:\n",
    "        # raise ValueError(\"‚ùå No .ntf files found!\")\n",
    "        print(\"‚ùå No .ntf files found!\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(ntf_files)} NTF files.\")\n",
    "    \n",
    "    # Store metadata values\n",
    "    metadata_values = {}\n",
    "    \n",
    "    # Extract metadata from each file\n",
    "    for ntf_file in ntf_files:\n",
    "        dataset = gdal.Open(ntf_file)\n",
    "        if dataset:\n",
    "            metadata = dataset.GetMetadata()\n",
    "            for key, value in metadata.items():\n",
    "                if value and value.strip():  # Remove empty or whitespace-only values\n",
    "                    if key not in metadata_values:\n",
    "                        metadata_values[key] = []\n",
    "                    metadata_values[key].append(value.strip())\n",
    "            dataset = None  # Close file\n",
    "    \n",
    "    # Function to aggregate metadata\n",
    "    aggregated_metadata = {}\n",
    "    for key, values in metadata_values.items():\n",
    "        try:\n",
    "            # If the key ends with \"_DATE\", use the minimum date\n",
    "            if key.upper().endswith(\"_DATE\"):\n",
    "                aggregated_metadata[key] = min(values)\n",
    "            else:\n",
    "                # Convert values to float if possible, then take the median\n",
    "                numeric_values = [float(v) for v in values if v.replace('.', '', 1).isdigit()]\n",
    "                if numeric_values:\n",
    "                    aggregated_metadata[key] = str(np.median(numeric_values))\n",
    "                else:\n",
    "                    # Take the lexicographical minimum for non-numeric values\n",
    "                    aggregated_metadata[key] = min(values)\n",
    "        except ValueError:\n",
    "            # Take minimum for categorical values\n",
    "            aggregated_metadata[key] = min(values)\n",
    "    \n",
    "    # print(\"‚úÖ Aggregated Metadata:\")\n",
    "    # for k, v in aggregated_metadata.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "    \n",
    "    # source_metadata = aggregated_metadata.get(\"NITF_PIAIMC_SOURCE\", \"Unknown\")\n",
    "    source_metadata = aggregated_metadata.get(\"NITF_PIAIMC_SOURCE\", \"Unknown\")\n",
    "        \n",
    "        # Check if the source contains \"VNIR\"\n",
    "        # if \"VNIR\" in source_info:\n",
    "        #     print(f\"üìå {ntf_file}: {source_info}\")\n",
    "        # vnir_str = source_info.get(\"VNIR\", \"VNIR key not found\")\n",
    "    # Check if VNIR exists inside the metadata string\n",
    "    if \"VNIR:\" in source_metadata:\n",
    "        for line in source_metadata.split(\"\\n\"):  # Handle multi-line metadata\n",
    "            if line.startswith(\"VNIR:\"):\n",
    "                catid = line.split(\":\", 1)[1].strip()  # Get value after colon\n",
    "       \n",
    "        print(catid)\n",
    "    \n",
    "    sensor = aggregated_metadata.get(\"NITF_ISORCE\")\n",
    "    \n",
    "    # Find the minimum date\n",
    "    # if acquisition_dates:\n",
    "    #     min_date = min(acquisition_dates)  # Dates are stored as strings, lexicographically sorted works for YYYYMMDD\n",
    "    #     max_date = max(acquisition_dates)  # Dates are stored as strings, lexicographically sorted works for YYYYMMDD\n",
    "    #     print(f\"‚úÖ Earliest acquisition date: {min_date}\")\n",
    "    #     print(f\"‚úÖ   Latest acquisition date: {max_date}\")\n",
    "    # else:\n",
    "    #     print(\"‚ùå No acquisition dates found in metadata!\")\n",
    "    \n",
    "    min_acquisition_date = aggregated_metadata.get(\"NITF_STDIDC_ACQUISITION_DATE\")\n",
    "    min_acquisition_date = f\"{min_acquisition_date[:8]}_{min_acquisition_date[8:]}\"\n",
    "    print(f\"‚úÖ Earliest acquisition date: {min_acquisition_date}\")\n",
    "    \n",
    "    # # If only one unique VNIR value exists, print it\n",
    "    # if len(vnir_counter) == 1:\n",
    "    #     catid = next(iter(vnir_counter))  # Get the single VNIR value\n",
    "    #     print(f\"Catalog ID: {catid}\")\n",
    "    # else:\n",
    "    #     print(\"Multiple catalog ids, exiting.\")\n",
    "    #     sys.exit()  # Exit without printing anything\n",
    "    print(f\"Catalog ID: {vnir_value}\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    output_base = f\"MS4_2m_{min_acquisition_date}_{sensor}_{catid}_v{timestamp}\"\n",
    "    output_vrt = os.path.join(output_dir, \"vrt\", f\"{output_base}.vrt\")\n",
    "    output_cog = os.path.join(output_dir, f\"{output_base}.tif\")\n",
    "    \n",
    "    print(f\"Output COG Filename: {output_cog}\")\n",
    "    \n",
    "    # # Function to determine the number of bands in an NTF file\n",
    "    # def get_num_bands(ntf_file):\n",
    "    #     dataset = gdal.Open(ntf_file)\n",
    "    #     if dataset:\n",
    "    #         num_bands = dataset.RasterCount\n",
    "    #         dataset = None  # Close the dataset\n",
    "    #         return num_bands\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"‚ùå Could not open {ntf_file}\")\n",
    "    \n",
    "    # # Check the band count of the first NTF file (assumes all have the same structure)\n",
    "    # num_bands = get_num_bands(ntf_files[0])\n",
    "    # print(f\"‚úÖ Detected {num_bands} bands.\")\n",
    "    \n",
    "    # # Define which bands to process\n",
    "    # bands_to_process = list(range(1, 5)) if num_bands == 4 else [2, 3, 5, 7]\n",
    "    \n",
    "    # # Apply aggregated metadata to output COG\n",
    "    # output_cog = os.path.join(output_dir, output_name)  # Output COG filename\n",
    "    \n",
    "    # gdal.SetConfigOption(\"GDAL_TIFF_INTERNAL_MASK\", \"YES\")  # Enable internal mask support\n",
    "    gdal.SetConfigOption(\"GDAL_CACHEMAX\", \"1536\")\n",
    "    \n",
    "    vrt_options = gdal.BuildVRTOptions(\n",
    "        separate=False\n",
    "    )\n",
    "    \n",
    "    translate_options = gdal.TranslateOptions(\n",
    "        format=\"COG\",  # Output format is COG\n",
    "        creationOptions=[\"NUM_THREADS=20\",\n",
    "                         \"PREDICTOR=2\",  # Apply LZW compression predictor\n",
    "                         \"BIGTIFF=IF_SAFER\",  # Enable BigTIFF support\n",
    "                         \"OVERVIEW_RESAMPLING=AVERAGE\",\n",
    "                         \"OVERVIEWS=IGNORE_EXISTING\"],\n",
    "        stats=False\n",
    "    )\n",
    "    \n",
    "    # warp_options = gdal.WarpOptions(\n",
    "    #     dstSRS=\"EPSG:3338\",  # Set target spatial reference\n",
    "    #     srcBands=bands_to_process,\n",
    "    #     multithread=True,\n",
    "    #     warpOptions=[\"NUM_THREADS=20\"],\n",
    "    #     creationOptions=[\"NUM_THREADS=20\",\n",
    "    #                      \"PREDICTOR=2\",  # Apply LZW compression predictor\n",
    "    #                      \"BIGTIFF=IF_SAFER\"],  # Enable BigTIFF support\n",
    "    #     # \"--config\", \"GDAL_CACHEMAX 1536\",\n",
    "    #     warpMemoryLimit=1536,\n",
    "    #     rpc=True,  # Use Rational Polynomial Coefficients\n",
    "    #     transformerOptions=[f\"RPC_DEM={dem_tif}\"],  # Use the specified DEM\n",
    "    #     srcNodata=0,\n",
    "    #     format=\"COG\",  # Output format is COG\n",
    "    #     xRes=2,\n",
    "    #     yRes=2,\n",
    "    #     targetAlignedPixels=True,\n",
    "    #     resampleAlg=\"cubic\",  # Resampling method\n",
    "    #     # metadataOptions=aggregated_metadata  # Apply aggregated metadata\n",
    "    #     copyMetadata=True)\n",
    "    \n",
    "    # # Run gdal.Warp with aggregated metadata\n",
    "    # gdal.Warp(output_cog, ntf_files, options=warp_options)\n",
    "\n",
    "    gdal.BuildVRT(\n",
    "        output_vrt,\n",
    "        row['input_path'],\n",
    "        options=vrt_options\n",
    "    )\n",
    "\n",
    "    gdal.Translate(\n",
    "        output_cog,\n",
    "        output_vrt,\n",
    "        options=translate_options\n",
    "    )\n",
    "    \n",
    "    # # Construct the gdalinfo command\n",
    "    info_cmd = [\n",
    "        conda_path, \"run\", \"-n\", conda_env,\n",
    "        # \"gdalwarp\",\n",
    "        \"/home/mmacander/miniconda3/envs/cog/bin/gdalinfo\",\n",
    "        \"-stats\",\n",
    "        \"-hist\"\n",
    "    ]\n",
    "    \n",
    "    info_cmd.extend([output_cog])\n",
    "    \n",
    "    # # Print and execute the command\n",
    "    print(f\"üöÄ Running: {' '.join(info_cmd)}\")\n",
    "    # subprocess.run(info_cmd, shell=True, executable=\"/bin/bash\", check=True)\n",
    "    subprocess.run(info_cmd, check=True)\n",
    "    \n",
    "    print(f\"‚úÖ Orthorectified COG saved with metadata, stats: {output_cog}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3fd21-a8d8-48bf-ad4d-b5ff4d86bd76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cog",
   "language": "python",
   "name": "cog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
